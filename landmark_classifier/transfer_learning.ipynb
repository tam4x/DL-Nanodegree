{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for Landmark Classification\n",
    "\n",
    "\n",
    "### Transfer learning\n",
    "\n",
    "In the previous notebook we have trained our own CNN and we got a certain performance. Let's see how hard it is to match that performance with transfer learning.\n",
    "\n",
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 0: Setting up\n",
    "\n",
    "The following cells make sure that your environment is setup correctly and check that your GPU is available and ready to go. You have to execute them every time you restart your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r requirements.txt | grep -v \"already satisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers import setup_env\n",
    "\n",
    "# If running locally, this will download dataset (make sure you have at \n",
    "# least 2 Gb of space on your hard drive)\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 1: Create transfer learning architecture\n",
    "\n",
    "Open the file `src/transfer.py` and complete the `get_model_transfer_learning` function. When you are done, execute this test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -vv src/transfer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 2: Train, validation and test\n",
    "\n",
    "Let's train our transfer learning model! Let's start defining the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # size of the minibatch for stochastic gradient descent (or Adam)\n",
    "valid_size = 0.2  # fraction of the training data to reserve for validation\n",
    "num_epochs = 50  # number of epochs for training\n",
    "num_classes = 50  # number of classes. Do not change this\n",
    "learning_rate = 0.001  # Learning rate for SGD (or Adam)\n",
    "opt = 'adam'      # optimizer. 'sgd' or 'adam'\n",
    "weight_decay = 0.0 # regularization. Increase this to combat overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import get_data_loaders\n",
    "from src.optimization import get_optimizer, get_loss\n",
    "from src.train import optimize\n",
    "from src.transfer import get_model_transfer_learning\n",
    "\n",
    "# Get a model using get_model_transfer_learning. Use one of the names reported here:\n",
    "# https://pytorch.org/vision/0.10/models.html\n",
    "# For example, if you want to load ResNet 18, use \"resnet18\"\n",
    "# NOTE: use the hyperparameters defined in the previous cell, do NOT copy/paste the\n",
    "# values\n",
    "\n",
    "model_transfer = get_model_transfer_learning(model_name = \"resnet18\")\n",
    "\n",
    "# train the model\n",
    "data_loaders = get_data_loaders(batch_size=batch_size)\n",
    "optimizer = get_optimizer(\n",
    "    model_transfer,\n",
    "    learning_rate=learning_rate,\n",
    "    optimizer=opt,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "loss = get_loss()\n",
    "\n",
    "optimize(\n",
    "    data_loaders,\n",
    "    model_transfer,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    n_epochs=num_epochs,\n",
    "    save_path=\"checkpoints/model_transfer.pt\",\n",
    "    interactive_tracking=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static_images/icons/noun-question-mark-869751.png\" alt=\"?\" style=\"width:25px\"/> __Question:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static_images/icons/noun-answer-3361020.png\" alt=\">\" style=\"width:25px\"/>  __Answer:__ I decided to use ResNet18 for the base of my model, since it performs fairly well on ImageNet and is not too large of a model. Also, since ResNet18 was trained for the ImageNet task, it is a good model to use for this landmark classificaiton task, since both ImageNet and this landmark task use images of natural scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play with the hyperparameters and see which performance you can get on the validation set. You should get at least 60% for a passing grade, but a good model choice and a good training strategy could get you up to 80% or so. Let's see how close you can get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 3: Test the Model\n",
    "\n",
    "Try out your model on the test dataset of landmark images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60% and matches more or less what you got on the validation set (otherwise you're overfitting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.train import one_epoch_test\n",
    "from src.transfer import get_model_transfer_learning\n",
    "\n",
    "model_transfer = get_model_transfer_learning(\"resnet18\", n_classes=num_classes)\n",
    "# Load saved weights\n",
    "model_transfer.load_state_dict(torch.load('checkpoints/model_transfer.pt'))\n",
    "\n",
    "one_epoch_test(data_loaders['test'], model_transfer, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"static_images/icons/noun-advance-2109145.png\" alt=\">\" style=\"width:50px\"/> Step 4: Export using torchscript\n",
    "\n",
    "Now, just like we did with our original model, we export the best fit model using torchscript so that it can be used in our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predictor import Predictor\n",
    "from src.helpers import compute_mean_and_std\n",
    "\n",
    "# First let's get the class names from our data loaders\n",
    "class_names = data_loaders[\"train\"].dataset.classes\n",
    "\n",
    "# Then let's move the model_transfer to the CPU\n",
    "# (we don't need GPU for inference)\n",
    "model_transfer = model_transfer.cpu()\n",
    "# Let's make sure we use the right weights by loading the\n",
    "# best weights we have found during training\n",
    "# NOTE: remember to use map_location='cpu' so the weights\n",
    "# are loaded on the CPU (and not the GPU)\n",
    "model_transfer.load_state_dict(\n",
    "    torch.load(\"checkpoints/model_transfer.pt\", map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "# Let's wrap our model using the predictor class\n",
    "mean, std = compute_mean_and_std()\n",
    "predictor = Predictor(model_transfer, class_names, mean, std).cpu()\n",
    "\n",
    "# Export using torch.jit.script\n",
    "scripted_predictor = torch.jit.script(predictor)\n",
    "scripted_predictor.save(\"checkpoints/transfer_exported.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.predictor import predictor_test\n",
    "from src.helpers import plot_confusion_matrix\n",
    "\n",
    "model_reloaded = torch.jit.load(\"checkpoints/transfer_exported.pt\")\n",
    "\n",
    "pred, truth = predictor_test(data_loaders['test'], model_reloaded)\n",
    "\n",
    "plot_confusion_matrix(pred, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "abfc6702f247a3f1ccbe6c8aa191024622802b77d3471c24be18bdc8d6958935"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
